Automatically generated by Mendeley Desktop 1.19.8
Any changes to this file will be lost if it is regenerated by Mendeley.

BibTeX export options can be customized via Options -> BibTeX in Mendeley Desktop

@article{Sørensen2022,
abstract = {We exploit the power of convex analysis to synthesize and extend a range of important results concerning the additive random utility model of discrete choice. With no restrictions on the joint distribution of random utility components or the functional form of systematic utility components, we formulate general versions of the Williams–Daly–Zachary theorem for demand and the Hotz–Miller demand inversion theorem. Based on these theorems, we provide necessary and sufficient conditions for demand and its inverse to reduce to functions. These conditions jointly imply that demand is a continuous function with a continuous inverse.},
author = {S{\o}rensen, Jesper R.V. and Fosgerau, Mogens},
doi = {10.1016/j.jmateco.2021.102629},
file = {:home/stefan/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/S{\o}rensen, Fosgerau - 2022 - How McFadden met Rockafellar and learned to do more with less.pdf:pdf},
issn = {18731538},
journal = {Journal of Mathematical Economics},
keywords = {Additive random utility model,Convex duality,Demand inversion,Discrete choice,Partial identification},
pages = {102629},
publisher = {Elsevier B.V.},
title = {{How McFadden met Rockafellar and learned to do more with less}},
url = {https://doi.org/10.1016/j.jmateco.2021.102629},
volume = {100},
year = {2022}
}
@unpublished{sasaki2019,
archivePrefix = {arXiv},
arxivId = {arXiv:1808.09375v2},
author = {Kato, Kengo and Sasaki, Yuya and Ura, Takuya},
booktitle = {arXiv Working Paper},
eprint = {arXiv:1808.09375v2},
file = {:home/stefan/Dropbox/Academia/Library/Kato, Sasaki, Ura{\_}2018{\_}Inference based on Kotlarski's Identity.pdf:pdf},
keywords = {deconvolution,kotlarski,measurement error,uniform confidence band},
pages = {1--30},
title = {{Inference based on Kotlarski's Identity}},
year = {2018}
}
@article{Bierens1982,
abstract = {In this paper we propose two consistent tests for functional form of nonlinear regression models without employing specified alternative models. The null hypothesis is that the regression function equals the conditional expectation function, which is tested against the alternative hypothesis that the null is false. These tests are based on a Fourier transform characterization of conditional expectations. {\textcopyright} 1982.},
author = {Bierens, Herman J.},
doi = {10.1016/0304-4076(82)90105-1},
file = {:home/stefan/Downloads/1-s2.0-0304407682901051-main.pdf:pdf},
issn = {03044076},
journal = {Journal of Econometrics},
number = {1},
pages = {105--134},
title = {{Consistent model specification tests}},
volume = {20},
year = {1982}
}
@phdthesis{Liang2022,
author = {Liang, Xiaoran},
file = {:home/stefan/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Liang - 2022 - Methods for Selecting Valid Instrumental Variables.pdf:pdf},
pages = {1--144},
title = {{Methods for Selecting Valid Instrumental Variables}},
year = {2022}
}
@article{Gautier,
author = {Gautier, Eric and Hoderlein, Stefan},
file = {:home/stefan/Dropbox/Academia/Library/Gautier, Hoderlein{\_}Unknown{\_}Coefficients in the Selection Equation.pdf:pdf},
keywords = {2011 cirm new trends,2012,chicago,college,crest,deconvolution,endogeneity,harvard-mit,ill-posed inverse problems,in mathematical statistics,kyoto,nanterre,nonparametric identification,north-,oxford,partial identification,princeton,radon transform,random coefficients,rates of convergence,roy model,seminar participants at boston,toulouse,treatment effects,ucl,vanderbilt,we are grateful to,western},
pages = {1--66},
title = {{Coefficients in the Selection Equation}}
}
@article{Delecroix2003,
abstract = {Semiparametric single-index regression involves an unknown finite-dimensional parameter and an unknown (link) function. We consider estimation of the parameter via the pseudo-maximum likelihood method. For this purpose we estimate the conditional density of the response given a candidate index and maximize the obtained likelihood. We show that this technique of adaptation yields an asymptotically efficient estimator: it has minimal variance among all estimators. {\textcopyright} 2003 Elsevier Science (USA). All rights reserved.},
author = {Delecroix, Michel and H{\"{a}}rdle, Wolfgang and Hristache, Marian},
doi = {10.1016/S0047-259X(02)00046-5},
file = {:home/stefan/Dropbox/Academia/Library/Delecroix, H{\"{a}}rdle, Hristache{\_}2003{\_}Efficient estimation in conditional single-index regression.pdf:pdf},
issn = {0047259X},
journal = {Journal of Multivariate Analysis},
keywords = {Pseudo-maximum likelihood,Semiparametric efficiency bound,Single-index model},
number = {2},
pages = {213--226},
title = {{Efficient estimation in conditional single-index regression}},
volume = {86},
year = {2003}
}
@techreport{Luksan2007,
author = {Luksan, Ladislav and Matonoha, Ctirad and Vlcek, Jan},
file = {:home/stefan/Dropbox/Academia/Library/Luksan, Matonoha, Vlcek{\_}2007{\_}New subroutines for large-scale optimization.pdf:pdf},
title = {{New subroutines for large-scale optimization}},
url = {http://scholar.google.com/scholar?hl=en{\&}btnG=Search{\&}q=intitle:New+subroutines+for+large-scale+optimization{\#}0},
year = {2007}
}
@article{Qu2015,
abstract = {This paper presents estimation methods and asymptotic theory for the analysis of a nonparametrically specified conditional quantile process. Two estimators based on local linear regressions are proposed. The first estimator applies simple inequality constraints while the second uses rearrangement to maintain quantile monotonicity. The bandwidth parameter is allowed to vary across quantiles to adapt to data sparsity. For inference, the paper first establishes a uniform Bahadur representation and then shows that the two estimators converge weakly to the same limiting Gaussian process. As an empirical illustration, the paper considers a dataset from Project STAR and delivers two new findings.},
author = {Qu, Zhongjun and Yoon, Jungmo},
doi = {10.1016/j.jeconom.2014.10.008},
file = {:home/stefan/Dropbox/Academia/Library/Qu, Yoon{\_}2015{\_}Nonparametric estimation and inference on conditional quantile processes.pdf:pdf},
issn = {18726895},
journal = {Journal of Econometrics},
keywords = {Nonparametric quantile regression,Treatment effect,Uniform Bahadur representation,Uniform inference},
number = {1},
pages = {1--19},
title = {{Nonparametric estimation and inference on conditional quantile processes}},
volume = {185},
year = {2015}
}
@article{Ledoit2017,
abstract = {Markowitz (1952) portfolio selection requires an estimator of the covariance matrix of returns. To address this problem, we promote a nonlinear shrinkage estimator that is more flexible than previous linear shrinkage estimators and has just the right number of free parameters (i.e., the Goldilocks principle). This number is the same as the number of assets. Our nonlinear shrinkage estimator is asymptotically optimal for portfolio selection when the number of assets is of the same magnitude as the sample size. In backtests with historical stock return data, it performs better than previous proposals and, in particular, it dominates linear shrinkage. (JEL C13, C58, G11)},
author = {Ledoit, Olivier and Wolf, Michael},
doi = {10.1093/rfs/hhx052},
file = {:home/stefan/Dropbox/Academia/Library/Ledoit, Wolf{\_}2017{\_}Nonlinear shrinkage of the covariance matrix for portfolio selection Markowitz meets goldilocks.pdf:pdf},
issn = {14657368},
journal = {Review of Financial Studies},
number = {12},
pages = {4349--4388},
title = {{Nonlinear shrinkage of the covariance matrix for portfolio selection: Markowitz meets goldilocks}},
volume = {30},
year = {2017}
}
@article{Westerlund2013,
author = {Westerlund, Joakim and Ests, T and Smeekes, Stephan},
file = {:home/stefan/Dropbox/Academia/Library/Westerlund, Ests, Smeekes{\_}2013{\_}Robust Block Bootstrap Panel Predictability Tests.pdf:pdf},
journal = {Maastricht University},
title = {{Robust Block Bootstrap Panel Predictability Tests}},
year = {2013}
}
@article{Gil-Pelaez1951,
abstract = {This is sufficient in the majority of cases, but when we are interested in the explicit expression of the distribution function, the constant F(0) is a substantial inconvenience. I therefore present a derivation of the inversion theorem by a new method which has the advantage of ...},
author = {Gil-Pelaez, J.},
doi = {10.2307/2332598},
issn = {00063444},
journal = {Biometrika},
number = {3/4},
pages = {481},
title = {{Note on the Inversion Theorem}},
volume = {38},
year = {1951}
}
@article{Bonhomme2015,
abstract = {This paper introduces time-varying grouped patterns of heterogeneity in linear panel data models. A distinctive feature of our approach is that group membership is left unrestricted. We estimate the parameters of the model using a “grouped fixed-effects” estimator that minimizes a least squares criterion with respect to all possible groupings of the cross-sectional units. Recent advances in the clustering literature allow for fast and efficient computation. We provide conditions under which our estimator is consistent as both dimensions of the panel tend to infinity, and we develop inference methods. Finally, we allow for grouped patterns of unobserved heterogeneity in the study of the link between income and democracy across countries.},
author = {Bonhomme, St{\'{e}}phane and Manresa, Elena},
doi = {10.3982/ecta11319},
file = {:home/stefan/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Bonhomme, Manresa - 2015 - Grouped Patterns of Heterogeneity in Panel Data.pdf:pdf},
issn = {14680262},
journal = {Econometrica},
number = {3},
pages = {1147--1184},
title = {{Grouped Patterns of Heterogeneity in Panel Data}},
volume = {83},
year = {2015}
}
@unpublished{Davezies2021,
abstract = {This article considers average marginal effects (AME) and similar parameters in a panel data fixed effects logit model. Relating the identified set of the AME to an extremal moment problem, we first show how to obtain sharp bounds on the AME straightforwardly, without any optimization. Then, we consider two strategies to build confidence intervals on the AME. In the first, we estimate the sharp bounds with a semiparametric two-step estimator involving a first-step nonparametric estimator. We derive the asymptotic distributions of these bounds under, mostly, a restriction on the cardinality of the support of the unobserved heterogeneity. The second, very simple strategy does not require any nonparametric estimation but may result in larger confidence intervals. Monte Carlo simulations suggest that both approaches work well in practice, the second being actually very competitive for usual sample sizes.},
archivePrefix = {arXiv},
arxivId = {2105.00879},
author = {Davezies, Laurent and D'Haultfoeuille, Xavier and Laage, Louise},
eprint = {2105.00879},
file = {:home/stefan/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Davezies, D'Haultfoeuille, Laage - 2021 - Identification and Estimation of average marginal effects in fixed effect logit models.pdf:pdf},
keywords = {c14,c23,c25,fixed effects logit models,jel codes,panel data,partial identification},
pages = {1--66},
title = {{Identification and Estimation of average marginal effects in fixed effect logit models}},
url = {http://arxiv.org/abs/2105.00879},
year = {2021}
}
@article{Badinger2008,
author = {Badinger, Harald and Egger, Peter},
file = {:home/stefan/Dropbox/Academia/Library/Badinger, Egger{\_}2008{\_}GM Estimation of Higher-Order Spatial Autoregressive Processes in Cross-Section Models with Heteroskedastic Disturb.pdf:pdf},
keywords = {CESifo Working Paper no. 2356,higher-order spatia},
title = {{GM Estimation of Higher-Order Spatial Autoregressive Processes in Cross-Section Models with Heteroskedastic Disturbances GM Estimation of Higher-Order Spatial Autoregressive Processes in Cross-Section Models with Heteroskedastic Disturbances Abstract}},
year = {2008}
}
@article{Abadie2014,
abstract = {When a researcher estimates the parameters of a regression function using information on all 50 states in the United States, or information on all visits to a website, what is the interpretation of the standard errors? Researchers typically report standard errors that are designed to capture sampling variation, based on viewing the data as a random sample drawn from a large population of interest, even in applications where it is difficult to articulate what that population of interest is and how it differs from the sample. In this paper we explore alternative interpretations for the uncertainty associated with regression estimates. As a leading example we focus on the case where some parameters of the regression function are intended to capture causal effects. We derive standard errors for causal effects using a generalization of randomization inference. Intuitively, these standard errors capture the fact that even if we observe outcomes for all units in the population of interest, there are for each unit missing potential outcomes for the treatment levels the unit was not exposed to. We show that our randomization-based standard errors in general are smaller than the conventional robust standard errors, and provide conditions under which they agree with them. More generally, correct statistical inference requires precise characterizations of the population of interest, the parameters that we aim to estimate within such population, and the sampling process. Estimation of causal parameters is one example where appropriate inferential methods may differ from conventional practice, but there are others.},
author = {Abadie, Alberto and Athey, Susan and Imbens, Guido W. and Wooldridge, Jeffrey M.},
doi = {10.3386/w20325},
file = {:home/stefan/Dropbox/Academia/Library/Abadie et al.{\_}2014{\_}Finite Population Causal Standard Errors.pdf:pdf},
journal = {National Bureau of Economic Research},
keywords = {Industrial Organization,Labor Studies,Public Eco},
title = {{Finite Population Causal Standard Errors}},
url = {http://www.princeton.edu/{~}erp/erp seminar pdfs/Imbens paper finite{\_}14apr12.pdf},
year = {2014}
}
@article{Bercu2007a,
abstract = {We investigate the asymptotic behavior of weighted sums of independent standardized random variables with uniformly bounded third moments. The sequence of weights is given by a family of rectangular matrices with uniformly small entries and approximately orthogonal rows. We prove that the empirical CDF of the resulting partial sums converges to the normal CDF with probability one. This result implies almost sure convergence of empirical periodograms, almost sure convergence of spectral distribution of circulant and reverse circulant matrices, and almost sure convergence of the CDF generated from independent random variables by independent random orthogonal matrices. In the special case of trigonometric weights, the speed of the almost sure convergence is described by a normal approximation as well as a large deviation principle. {\textcopyright} 2007 Applied Probability Trust.},
author = {Bercu, Bernard and Bryc, W{\l}odzimierz},
doi = {10.1214/ECP.v12-1273},
file = {:home/stefan/Dropbox/Academia/Library/Bercu, Bryc{\_}2007{\_}Asymptotic results for empiricalmeasures of weighted sums of independent random variables.pdf:pdf},
issn = {1083589X},
journal = {arXiv Working Paper},
keywords = {Almost sure central limit theorem,Large deviations,Normal approximation,Peri-odogram},
pages = {184--199},
title = {{Asymptotic results for empiricalmeasures of weighted sums of independent random variables}},
url = {http://www.emis.ams.org/journals/EJP-ECP/{\_}ejpecp/ECP/include/getdoce15b.pdf?id=4189{\&}article=1882{\&}mode=pdf},
volume = {12},
year = {2007}
}
@article{Tuvaandorj2014,
abstract = {We consider conditional distribution and conditional density functionals in the space of generalized functions. The approach follows Phillips (1985, 1991, 1995) who employed generalized functions to overcome non-differentiability in order to develop expansions. We obtain the limit of the kernel estimators for weakly dependent data, even under non-differentiability of the distribution function; the limit Gaussian process is characterized as a stochastic random functional (random generalized function) on the suitable function space. An alternative simple to compute estimator based on the empirical distribution function is proposed for the generalized random functional. For test statistics based on this estimator, limit properties are established. A Monte Carlo experiment demonstrates good finite sample performance of the statistics for testing logit and probit specification in binary choice models.},
author = {Tuvaandorj, Purevdorj and Zinde-Walsh, Victoria},
doi = {10.1108/S0731-905320140000033012},
file = {:home/stefan/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Tuvaandorj, Zinde-Walsh - 2014 - Limit theory and inference about conditional distributions.pdf:pdf},
issn = {07319053},
journal = {Advances in Econometrics},
keywords = {Bootstrap,Conditional distribution,Empirical distribution function,Generalized functions,Specification testing},
pages = {397--423},
title = {{Limit theory and inference about conditional distributions}},
volume = {33},
year = {2014}
}
@article{Fan1991,
abstract = {Deconvolution problems arise in a variety of situations in statistics. An interesting problem is to estimate the density f of a random variable X based on n i.i.d. observations from Y = X + $\epsilon$, where $\epsilon$ is a measurement error with a known distribution. In this paper, the effect of errors in variables of nonparametric deconvolution is examined. Insights are gained by showing that the difficulty of deconvolution depends on the smoothness of error distributions: the smoother, the harder. In fact, there are two types of optimal rates of convergence according to whether the error distribution is ordinary smooth or supersmooth. It is shown that optimal rates of convergence can be achieved by deconvolution kernel density estimators. CR - Copyright {\&}{\#}169; 1991 Institute of Mathematical Statistics},
author = {Fan, Jianqing},
doi = {10.1214/aos/1176348248},
file = {:home/stefan/Dropbox/Academia/Library/Fan{\_}1991{\_}On the Optimal Rates of Convergence for Nonparametric Deconvolution Problems.pdf:pdf},
issn = {0090-5364},
journal = {The Annals of Statistics},
number = {3},
pages = {1257--1272},
title = {{On the Optimal Rates of Convergence for Nonparametric Deconvolution Problems}},
volume = {19},
year = {1991}
}
@incollection{Lawson1995,
author = {Lawson, Charles L. and Hanson, Richard J.},
booktitle = {SIAM},
edition = {Classics i},
title = {{Solving Least Squares Problems}},
year = {1995}
}
@article{Hallin2009,
abstract = {Let {\{}(Y i, X i), i ∈ ℤ n{\}} be a stationary real-valued (d + 1)-dimensional spatial processes. Denote by x → qp(x). P ∈ (0,1), x ∈ ℝ d, the spatial quantile regression function of order p, characterized by P{\{}Y i[≤ qp(x)|X i=x{\}} = p. Assume that the process has been observed over an N-dimensional rectangular domain of the form I n : = {\{}i = (i 1...,i N) ∈ ℤ N|1 ≤ k ≤n k, k= l,...,N{\}}, with n = (n 1.....n N) ∈ℤ N. We propose a local linear estimator of q p. That estimator extends to random fields with unspecified and possibly highly complex spatial dependence structure, the quantile regression methods considered in the context of independent samples or time series. Under mild regularity assumptions, we obtain a Bahadur representation for the estimators of q p and its first-order derivatives, from which we establish consistency and asymptotic normality. The spatial process is assumed to satisfy general mixing conditions, generalizing classical time series mixing concepts. The size of the rectangular domain I n is allowed to tend to infinity at different rates depending on the direction in ℤ N (non-isotropic asymptotics). The method provides much richer information than the mean regression approach considered in most spatial modelling techniques. {\textcopyright} 2009 ISI/BS.},
author = {Hallin, Marc and Lu, Zudi and Yu, Keming},
doi = {10.3150/08-BEJ168},
file = {:home/stefan/Dropbox/Academia/Library/Hallin, Lu, Yu{\_}2009{\_}Local linear spatial quantile regression.pdf:pdf},
issn = {13507265},
journal = {Bernoulli},
keywords = {Bahadur representation,Local linear estimation,Quantile regression,Random fields},
month = {aug},
number = {3},
pages = {659--686},
title = {{Local linear spatial quantile regression}},
url = {http://projecteuclid.org/euclid.bj/1251463276},
volume = {15},
year = {2009}
}
@article{Andrews2013,
abstract = {In this paper, we propose an instrumental variable approach to constructing confi dence sets (CS's) for the true parameter in models defined by conditional moment in equalities/equalities. We show that by properly choosing instrument functions, one can transform conditional moment inequalities/equalities into unconditional ones without losing identification power. Based on the unconditional moment inequalities/equalities, we construct CS's by inverting Cram{\'{e}}r-von Mises-type or Kolmogorov-Smirnov-type tests. Critical values are obtained using generalized moment selection (GMS) proce dures. We show that the proposed CS's have correct uniform asymptotic coverage prob abilities. New methods are required to establish these results because an infinite dimensional nuisance parameter affects the asymptotic distributions. We show that the tests considered are consistent against all fixed alternatives and typically have power against «{\~{}}1/2-local alternatives to some, but not all, sequences of distributions in the null hypothesis. Monte Carlo simulations for five different models show that the meth ods perform well in finite sam},
author = {Andrews, Donald W.K. and Shi, Xiaoxia},
doi = {10.1016/j.jeconom.2016.09.010},
file = {:home/stefan/Dropbox/Academia/Library/Andrews, Shi{\_}2013{\_}Inference based on many conditional moment inequalities.pdf:pdf},
issn = {18726895},
journal = {Econometrica},
keywords = {Asymptotic size,Cram{\'{e}}r-von Mises,Kolmogorov,Smirnov,asymptotic power,conditional moment inequali,confidence set,generalized moment selection,moment inequal,ties},
number = {2},
pages = {609--666},
title = {{Inference based on many conditional moment inequalities}},
volume = {81},
year = {2013}
}
@article{Chernozhukov2018,
abstract = {This paper considers the problem of testing many moment inequalities where the number of moment inequalities, denoted by {\$}p{\$}, is possibly much larger than the sample size {\$}n{\$}. There is a variety of economic applications where solving this problem allows to carry out inference on causal and structural parameters, a notable example is the market structure model of Ciliberto and Tamer (2009) where {\$}p=2{\^{}}{\{}m+1{\}}{\$} with {\$}m{\$} being the number of firms that could possibly enter the market. We consider the test statistic given by the maximum of {\$}p{\$} Studentized (or {\$}t{\$}-type) inequality-specific statistics, and analyze various ways to compute critical values for the test statistic. Specifically, we consider critical values based upon (i) the union bound combined with a moderate deviation inequality for self-normalized sums, (ii) the multiplier and empirical bootstraps, and (iii) two-step and three-step variants of (i) and (ii) by incorporating the selection of uninformative inequalities that are far from being binding and a novel selection of weakly informative inequalities that are potentially binding but do not provide first order information. We prove validity of these methods, showing that under mild conditions, they lead to tests with the error in size decreasing polynomially in {\$}n{\$} while allowing for {\$}p{\$} being much larger than {\$}n{\$}, indeed {\$}p{\$} can be of order {\$}\backslashexp (n{\^{}}{\{}c{\}}){\$} for some {\$}c {\textgreater} 0{\$}. Importantly, all these results hold without any restriction on the correlation structure between {\$}p{\$} Studentized statistics, and also hold uniformly with respect to suitably large classes of underlying distributions. Moreover, in the online supplement, we show validity of a test based on the block multiplier bootstrap in the case of dependent data under some general mixing conditions.},
archivePrefix = {arXiv},
arxivId = {arXiv:1312.7614v6},
author = {Chernozhukov, Victor and Chetverikov, Denis and Kato, Kengo},
doi = {10.1093/restud/rdy065},
eprint = {arXiv:1312.7614v6},
file = {:home/stefan/Dropbox/Academia/Library/Chernozhukov, Chetverikov, Kato{\_}2018{\_}Inference on Causal and Structural Parameters using Many Moment Inequalities.pdf:pdf},
issn = {0034-6527},
journal = {The Review of Economic Studies},
keywords = {adam rosen,and phrases,asian meeting of econometric,azeem shaikh,bernoulli society,empirical bootstrap,jin hahn,many moment inequalities,moderate deviation,multiplier and,non-asymptotic bound,participants at cowles,self-normalized sum,society 2013,summer conference 2013,we are grateful to},
pages = {1--83},
title = {{Inference on Causal and Structural Parameters using Many Moment Inequalities}},
year = {2018}
}
@article{Kang2016,
abstract = {Instrumental variables have been widely used for estimating the causal effect between exposure and outcome. Conventional estimation methods require complete knowledge about all the instruments' validity; a valid instrument must not have a direct effect on the outcome and not be related to unmeasured confounders. Often, this is impractical as highlighted by Mendelian randomization studies where genetic markers are used as instruments and complete knowledge about instruments' validity is equivalent to complete knowledge about the involved genes' functions. In this article, we propose a method for estimation of causal effects when this complete knowledge is absent. It is shown that causal effects are identified and can be estimated as long as less than 50{\%} of instruments are invalid, without knowing which of the instruments are invalid. We also introduce conditions for identification when the 50{\%} threshold is violated. A fast penalized ℓ1estimation method, called sisVIVE, is introduced for estimating the causal effect without knowing which instruments are valid, with theoretical guarantees on its performance. The proposed method is demonstrated on simulated data and a real Mendelian randomization study concerning the effect of body mass index(BMI) on health-related quality of life (HRQL) index. An R package sisVIVE is available on CRAN. Supplementary materials for this article are available online.},
archivePrefix = {arXiv},
arxivId = {1401.5755},
author = {Kang, Hyunseung and Zhang, Anru and Cai, T. Tony and Small, Dylan S.},
doi = {10.1080/01621459.2014.994705},
eprint = {1401.5755},
file = {:home/stefan/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Kang et al. - 2016 - Instrumental Variables Estimation With Some Invalid Instruments and its Application to Mendelian Randomization.pdf:pdf},
issn = {1537274X},
journal = {Journal of the American Statistical Association},
keywords = {Body mass index,Causal inference,Health-related quality of life,Instrumental variable,Pleiotropy,ℓ1penalization},
number = {513},
pages = {132--144},
title = {{Instrumental Variables Estimation With Some Invalid Instruments and its Application to Mendelian Randomization}},
volume = {111},
year = {2016}
}
@article{Heckman2016,
author = {Heckman, James J and Vytlacil, Edward},
file = {:home/stefan/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Heckman, Vytlacil - 2016 - Policy-Relevant Treatment Effects.pdf:pdf},
number = {2},
pages = {107--111},
title = {{Policy-Relevant Treatment Effects}},
volume = {91},
year = {2016}
}
@article{Cosslett1997,
author = {Cosslett, Stephen R.},
doi = {10.1016/S0169-7161(97)15016-7},
file = {:home/stefan/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Cosslett - 1997 - 14 Nonparametric maximum likelihood methods.pdf:pdf},
issn = {01697161},
journal = {Handbook of Statistics},
pages = {385--404},
title = {{14 Nonparametric maximum likelihood methods}},
volume = {15},
year = {1997}
}
@article{Chernozhukov2018a,
abstract = {We revisit the classic semi-parametric problem of inference on a low-dimensional parameter $\theta$0 in the presence of high-dimensional nuisance parameters $\eta$0. We depart from the classical setting by allowing for $\eta$0 to be so high-dimensional that the traditional assumptions (e.g. Donsker properties) that limit complexity of the parameter space for this object break down. To estimate $\eta$0, we consider the use of statistical or machine learning (ML) methods, which are particularly well suited to estimation in modern, very high-dimensional cases. ML methods perform well by employing regularization to reduce variance and trading off regularization bias with overfitting in practice. However, both regularization bias and overfitting in estimating $\eta$0 cause a heavy bias in estimators of $\theta$0 that are obtained by naively plugging ML estimators of $\eta$0 into estimating equations for $\theta$0. This bias results in the naive estimator failing to be N-1/2 consistent, where N is the sample size. We show that the impact of regularization bias and overfitting on estimation of the parameter of interest $\theta$0 can be removed by using two simple, yet critical, ingredients: (1) using Neyman-orthogonal moments/scores that have reduced sensitivity with respect to nuisance parameters to estimate $\theta$0; (2) making use of cross-fitting, which provides an efficient form of data-splitting. We call the resulting set of methods double or debiased ML (DML). We verify that DML delivers point estimators that concentrate in an N-1 -neighbourhood of the true parameter values and are approximately unbiased and normally distributed, which allows construction of valid confidence statements. The generic statistical theory of DML is elementary and simultaneously relies on only weak theoretical requirements, which will admit the use of a broad array of modern ML methods for estimating the nuisance parameters, such as random forests, lasso, ridge, deep neural nets, boosted trees, and various hybrids and ensembles of these methods. We illustrate the general theory by applying it to provide theoretical properties of the following: DML applied to learn the main regression parameter in a partially linear regression model; DML applied to learn the coefficient on an endogenous variable in a partially linear instrumental variables model; DML applied to learn the average treatment effect and the average treatment effect on the treated under unconfoundedness; DML applied to learn the local average treatment effect in an instrumental variables setting. In addition to these theoretical applications, we also illustrate the use of DML in three empirical examples.},
author = {Chernozhukov, Victor and Chetverikov, Denis and Demirer, Mert and Duflo, Esther and Hansen, Christian and Newey, Whitney and Robins, James},
doi = {10.1111/ectj.12097},
file = {:home/stefan/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Chernozhukov et al. - 2018 - Doubledebiased machine learning for treatment and structural parameters.pdf:pdf},
issn = {1368423X},
journal = {Econometrics Journal},
number = {1},
pages = {C1--C68},
title = {{Double/debiased machine learning for treatment and structural parameters}},
volume = {21},
year = {2018}
}
@article{Franc2005,
author = {Franc, Vojtech and Hlavac, Vaclav and Navara, Mirko},
journal = {Computer Analysis of Images and Patterns},
pages = {407--414},
title = {{Sequential coordinate-wise algorithm for the non-negative least squares problem}},
url = {http://www.springerlink.com/index/epdjxebt42g3j5v2.pdf},
volume = {3691},
year = {2005}
}
@article{Escanciano2014,
abstract = {A new uniform expansion is introduced for sums of weighted kernel-based regression residuals from nonparametric or semiparametric models. This expansion is useful for deriving asymptotic properties of semiparametric estimators and test statistics with data-dependent bandwidths, random trimming, and estimated efficiency weights. Provided examples include a new estimator for a binary choice model with selection and an associated directional test for specification of this model's average structural function. An appendix contains new results on uniform rates for kernel estimators and primitive sufficient conditions for high level assumptions commonly used in semiparametric estimation. {\textcopyright} 2013 Elsevier B.V. All rights reserved.},
author = {Escanciano, Juan Carlos and Jacho-Ch{\'{a}}vez, David T. and Lewbel, Arthur},
doi = {10.1016/j.jeconom.2013.06.004},
file = {:home/stefan/Dropbox/Academia/Library/Escanciano, Jacho-Ch{\'{a}}vez, Lewbel{\_}2014{\_}Uniform convergence of weighted sums of non and semiparametric residuals for estimation and testi.pdf:pdf},
issn = {03044076},
journal = {Journal of Econometrics},
keywords = {Empirical process theory,Limited dependent variables,Nonparametric residuals,Sample selection models,Semiparametric regression,Semiparametric residuals,Uniform-in-bandwidth},
number = {PART 3},
pages = {426--443},
title = {{Uniform convergence of weighted sums of non and semiparametric residuals for estimation and testing}},
volume = {178},
year = {2014}
}
@article{Hansen2008,
abstract = {This paper presents a set of rate of uniform consistency results for kernel estimators of density functions and regressions functions. We generalize the existing literature by allowing for stationary strong mixing multivariate data with infinite support, kernels with unbounded support, and general bandwidth sequences. These results are useful for semiparametric estimation based on a first-stage nonparametric estimator. {\textcopyright} 2008 Cambridge University Press.},
author = {Hansen, Bruce E.},
doi = {10.1017/S0266466608080304},
file = {:home/stefan/Dropbox/Academia/Library/Hansen{\_}2008{\_}Uniform convergence rates for kernel estimation with dependent data.pdf:pdf},
issn = {02664666},
journal = {Econometric Theory},
number = {3},
pages = {726--748},
title = {{Uniform convergence rates for kernel estimation with dependent data}},
volume = {24},
year = {2008}
}
@article{Andrews2010,
abstract = {The topic of this paper is inference in models in which parameters are defined by moment inequalities and/or equalities. The parameters may or may not be identified. This paper introduces a new class of confidence sets and tests based on generalized moment selection (GMS). GMS procedures are shown to have correct asymptotic size in a uniform sense and are shown not to be asymptotically conservative. The power of GMS tests is compared to that of subsampling, m out of n bootstrap, and " plug-in asymptotic " (PA) tests. The latter three procedures are the only general procedures in the literature that have been shown to have correct asymptotic size (in a uniform sense) for the moment inequality/equality model. GMS tests are shown to have asymptotic power that dominates that of subsampling, m out of n bootstrap, and PA tests. Subsampling and m out of n bootstrap tests are shown to have asymptotic power that dominates that of PA tests. KEYWORDS: Asymptotic size, asymptotic power, confidence set, exact size, general-ized moment selection, m out of n bootstrap, subsampling, moment inequalities, mo-ment selection, test.},
author = {Andrews, Donald W.K. and Soares, Gustavo},
doi = {10.3982/ecta7502},
file = {:home/stefan/Dropbox/Academia/Library/Andrews, Soares{\_}2010{\_}Inference for Parameters Defined by Moment Inequalities Using Generalized Moment Selection.pdf:pdf},
issn = {0012-9682},
journal = {Econometrica},
number = {1},
pages = {119--157},
title = {{Inference for Parameters Defined by Moment Inequalities Using Generalized Moment Selection}},
volume = {78},
year = {2010}
}
@article{Goldberger1962,
author = {Goldberger, Arthur S.},
file = {:home/stefan/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Goldberger - 1962 - Best Linear Unbiased Prediction in the Generalized Linear Regression Model Author ( s ) Arthur S . Goldberger Review.pdf:pdf},
journal = {Journal of American Statistical Association},
number = {298},
pages = {369--375},
title = {{Best Linear Unbiased Prediction in the Generalized Linear Regression Model Author ( s ): Arthur S . Goldberger Reviewed work ( s ): Source : Journal of the American Statistical Association , Vol . 57 , No . 298 ( Jun ., 1962 ), pp . 369- Published by : Am}},
volume = {57},
year = {1962}
}
@article{Johansson2006,
author = {Johansson, Bjoern and Elfving, Tommy and Kozlov, Vladimir and Censor, Yair and Forssen, Per-Erik and Granlund, Goesta},
journal = {Mathematical and Computer Modelling},
keywords = {Johansson2006},
number = {7},
pages = {892--909},
title = {{The application of an oblique-projected Landweber method to a model of supervised learning}},
url = {http://www.sciencedirect.com/science/article/pii/S0895717705005297},
volume = {43},
year = {2006}
}
@article{Andrews1999,
abstract = {This paper considers a generalized method of moments (GMM) estimation problem in which one has a vector of moment conditions, some of which are correct and some incorrect. The paper introduces several procedures for consistently selecting the correct moment conditions. The procedures also can consistently determine whether there is a sufficient number of correct moment conditions to identify the unknown parameters of interest. The paper specifies moment selection criteria that are GMM analogues of the widely used BIC and AIC model selection criteria. (The latter is not consistent.) The paper also considers downward and upward testing procedures. All of the moment selection procedures discussed in this paper are based on the minimized values of the GMM criterion function for different vectors of moment conditions. The procedures are applicable in time-series and cross-sectional contexts. Application of the results of the paper to instrumental variables estimation problems yields consistent procedures for selecting instrumental variables.},
author = {Andrews, Donald W.K.},
doi = {10.1111/1468-0262.00036},
file = {:home/stefan/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Andrews - 1999 - Consistent moment selection procedures for generalized method of moments estimation.pdf:pdf},
issn = {00129682},
journal = {Econometrica},
keywords = {Akaike information criterion,Bayesian information criterion,Consistent selection procedure,Downward testing procedure,Generalized method of moments estimator,Instrumental variables estimator,Model selection,Moment selection},
number = {3},
pages = {543--563},
title = {{Consistent moment selection procedures for generalized method of moments estimation}},
volume = {67},
year = {1999}
}
@article{JosephP.2014,
abstract = {This paper considers the problem of testing a finite number of moment inequalities. We propose a two-step approach. In the first step, a confidence region for the moments is constructed. In the second step, this set is used to provide information about which moments are “negative.” A Bonferonni-type correction is used to account for the fact that with some probability the moments may not lie in the confidence region. It is shown that the test controls size uniformly over a large class of distributions for the observed data. An important feature of the proposal is that it remains computationally feasible, even when the number of moments is large. The finite-sample properties of the procedure are examined via a simulation study, which demonstrates, among other things, that the proposal remains competitive with existing procedures while being computationally more attractive.},
author = {Romano, Joseph P. and Shaikh, Azeem M. and Wolf, Michael},
doi = {10.3982/ecta11011},
file = {:home/stefan/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Romano, Shaikh, Wolf - 2014 - A Practical Two-Step Method for Testing Moment Inequalities.pdf:pdf},
issn = {0012-9682},
journal = {Econometrica},
number = {5},
pages = {1979--2002},
title = {{A Practical Two-Step Method for Testing Moment Inequalities}},
volume = {82},
year = {2014}
}
@article{Deaner2022,
abstract = {We apply results in Hu and Schennach (2008) to achieve nonparametric identification of causal effects using noisy proxies for unobserved confounders. We call this the `triple proxy' approach because it requires three proxies that are jointly independent conditional on unobservables. We consider three different choices for the third proxy: it may be an outcome, a vector of treatments, or a collection of auxiliary variables. We compare to an alternative identification strategy introduced by Miao et. al. (2018) in which causal effects are identified using two conditionally independent proxies. We refer to this as the `double proxy' approach. We show that the conditional independence assumptions in the double and triple proxy approaches are non-nested, which suggests that either of the two identification strategies may be appropriate depending on the particular setting.},
archivePrefix = {arXiv},
arxivId = {2204.13815},
author = {Deaner, Ben},
eprint = {2204.13815},
file = {:home/stefan/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Deaner - 2022 - Controlling for Latent Confounding with Triple Proxies.pdf:pdf},
pages = {1--28},
title = {{Controlling for Latent Confounding with Triple Proxies}},
url = {http://arxiv.org/abs/2204.13815},
year = {2022}
}
@article{Guo2018,
abstract = {A major challenge in instrumental variable (IV) analysis is to find instruments that are valid, or have no direct effect on the outcome and are ignorable. Typically one is unsure whether all of the putative IVs are in fact valid. We propose a general inference procedure in the presence of invalid IVs, called two-stage hard thresholding with voting. The procedure uses two hard thresholding steps to select strong instruments and to generate candidate sets of valid IVs. Voting takes the candidate sets and uses majority and plurality rules to determine the true set of valid IVs. In low dimensions with invalid instruments, our proposal correctly selects valid IVs, consistently estimates the causal effect, produces valid confidence intervals for the causal effect and has oracle optimal width, even if the so-called 50{\%} rule or the majority rule is violated. In high dimensions, we establish nearly identical results without oracle optimality. In simulations, our proposal outperforms traditional and recent methods in the invalid IV literature. We also apply our method to reanalyse the causal effect of education on earnings.},
archivePrefix = {arXiv},
arxivId = {1603.05224},
author = {Guo, Zijian and Kang, Hyunseung and {Tony Cai}, T. and Small, Dylan S.},
doi = {10.1111/rssb.12275},
eprint = {1603.05224},
file = {:home/stefan/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Guo et al. - 2018 - Confidence intervals for causal effects with invalid instruments by using two-stage hard thresholding with voting.pdf:pdf},
issn = {14679868},
journal = {Journal of the Royal Statistical Society. Series B: Statistical Methodology},
keywords = {Exclusion restriction,High dimensional covariates,Invalid instruments,Majority voting,Plurality voting,Treatment effect},
number = {4},
pages = {793--815},
title = {{Confidence intervals for causal effects with invalid instruments by using two-stage hard thresholding with voting}},
volume = {80},
year = {2018}
}
@article{Gautier2009,
abstract = {Nous consid$\backslash$'erons dans cet article des mod$\backslash$`eles $\backslash$`a choix binaires et coefficients al$\backslash$'eatoires. Le but est d'estimer de mani$\backslash$`ere nonparam$\backslash$'etrique la densit$\backslash$'e du coefficient al$\backslash$'eatoire. Il s'agit d'un probl$\backslash$`eme inverse mal pos$\backslash$'e caract$\backslash$'eris$\backslash$'e par une transformation int$\backslash$'egrale. Un nouvel estimateur de la densit$\backslash$'e du coefficient al$\backslash$'eatoire est propos$\backslash$'e. Il est bas$\backslash$'e sur les d$\backslash$'eveloppements en s$\backslash$'eries de Fourier-Laplace sur la sph$\backslash$`ere. Cette approche permet une $\backslash$'etude fine du probl$\backslash$`eme d'identification mais aussi d'obtenir un estimateur par injection ayant une expression explicite et ne n$\backslash$'ecessitant aucun optimisation num$\backslash$'erique. Le nouvel estimateur est donc tr$\backslash$`es facile $\backslash$`a obtenir num$\backslash$'eriquement, tout en $\backslash$'etant souple sur le traitement de l'h$\backslash$'et$\backslash$'erog$\backslash$'en$\backslash$'eit$\backslash$'e inobserv$\backslash$'ee. Nous pr$\backslash$'esentons des extensions parmi lesquellesle traitement de coefficients non al$\backslash$'eatoires et de mod$\backslash$`eles avec endog$\backslash$'en$\backslash$'eit$\backslash$'e.},
archivePrefix = {arXiv},
arxivId = {0907.2451},
author = {Gautier, Eric and Kitamura, Yuichi},
doi = {10.3982/ecta8675},
eprint = {0907.2451},
file = {:home/stefan/Dropbox/Academia/Library/Gautier, Kitamura{\_}2013{\_}Nonparametric Estimation in Random Coefficients Binary Choice Models.pdf:pdf},
issn = {0012-9682},
journal = {Econometrica},
month = {jul},
number = {2},
pages = {581--607},
title = {{Nonparametric Estimation in Random Coefficients Binary Choice Models}},
url = {http://arxiv.org/abs/0907.2451},
volume = {81},
year = {2013}
}
@article{Aquaro2010,
author = {Aquaro, M and {\v{C}}{\'{i}}{\'{z}}ek, Pavel},
file = {:home/stefan/Dropbox/Academia/Library/Aquaro, {\v{C}}{\'{i}}{\'{z}}ek{\_}2010{\_}One-step robust estimation of fixed-effects panel data models.pdf:pdf},
title = {{One-step robust estimation of fixed-effects panel data models}},
url = {http://papers.ssrn.com/sol3/papers.cfm?abstract{\_}id=1694496},
year = {2010}
}
@article{Feve2018,
abstract = {Consider a nonparametric nonseparable regression model Y = ϕ(Z, U), where ϕ(Z, U) is strictly increasing in U and U ∼ U[0, 1]. We suppose that there exists an instrument W that is independent of U. The observable random variables are Y, Z, and W, all one-dimensional. We construct test statistics for the hypothesis that Z is exogenous, that is, that U is independent of Z. The test statistics are based on the observation that Z is exogenous if and only if V = FY|Z(Y|Z) is independent of W, and hence they do not require the estimation of the function ϕ. The asymptotic properties of the proposed tests are proved, and a bootstrap approximation of the critical values of the tests is shown to be consistent and to work for finite samples via simulations. An empirical example using the U.K. Family Expenditure Survey is also given. As a byproduct of our results we obtain the asymptotic properties of a kernel estimator of the distribution of V, which equals U when Z is exogenous. We show that this estimator converges to the uniform distribution at faster rate than the parametric n− 1/2-rate.},
author = {F{\`{e}}ve, Fr{\'{e}}d{\'{e}}rique and Florens, Jean Pierre and {Van Keilegom}, Ingrid},
doi = {10.1080/07350015.2016.1166120},
file = {:home/stefan/Dropbox/Academia/Library/F{\`{e}}ve, Florens, Van Keilegom{\_}2018{\_}Estimation of Conditional Ranks and Tests of Exogeneity in Nonparametric Nonseparable Models.pdf:pdf},
issn = {15372707},
journal = {Journal of Business and Economic Statistics},
keywords = {Endogeneity,Instrumental variable,Nonparametric regression,Nonseparability},
number = {2},
pages = {334--345},
title = {{Estimation of Conditional Ranks and Tests of Exogeneity in Nonparametric Nonseparable Models}},
volume = {36},
year = {2018}
}
@article{Kong2010,
abstract = {We use local polynomial fitting to estimate the nonparametric M-regression function for strongly mixing stationary processes {\{}(Yi/, X i){\}}. We establish a strong uniform consistency rate for the Bahadur representation of estimators of the regression function and its derivatives. These results are fundamental for statistical inference and for applications that involve plugging such estimators into other functionals where some control over higher order terms is required. We apply our results to the estimation of an additive M-regression model. {\textcopyright} 2010 Cambridge University Press.},
archivePrefix = {arXiv},
arxivId = {0709.1663},
author = {Kong, Efang and Linton, Oliver and Xia, Yingcun},
doi = {10.1017/S0266466609990661},
eprint = {0709.1663},
file = {:home/stefan/Dropbox/Academia/Library/Kong, Linton, Xia{\_}2010{\_}Uniform Bahadur representation for local polynomial estimates of M-regression and its application to the additive.pdf:pdf},
issn = {02664666},
journal = {Econometric Theory},
number = {5},
pages = {1529--1564},
title = {{Uniform Bahadur representation for local polynomial estimates of M-regression and its application to the additive model}},
volume = {26},
year = {2010}
}
@article{Kitagawa2018,
abstract = {One of the main objectives of empirical analysis of experiments and quasi-experiments is to inform policy decisions that determine how treatments are allocated to individuals with different observable covariates. We propose the Empirical Welfare Maximization (EWM) method, which estimates a treatment assignment policy by maximizing the sample analog of average social welfare over a class of candidate treatment policies. We show that, when propensity score is known, the average social welfare attained by EWM rules converges at least at n{\{}{\^{}}{\}}{\{}{\{}{\}}-1/2{\{}{\}}{\}} rate to the maximum obtainable welfare. This holds uniformly over a minimally constrained class of data distributions, and this uniform convergence rate is minimax optimal. In comparison with this benchmark rate, we examine how the uniform convergence rate of the average welfare improves or deteriorates depending on the richness of the class of candidate decision rules, on the distribution of conditional treatment effects, on the lack of knowledge for the propensity score, and on additional smoothness assumptions for the regression functions or propensity scores. We also discuss practically implementable computation for the EWM rule. As an empirical application, we derive an EWM rule for the a training program using the experiment data analyzed in La Londe (1986).},
author = {Kitagawa, Toru and Tetenov, Aleksey},
doi = {10.3982/ecta13288},
file = {:home/stefan/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Kitagawa, Tetenov - 2018 - Who Should Be Treated Empirical Welfare Maximization Methods for Treatment Choice.pdf:pdf},
issn = {0012-9682},
journal = {Econometrica},
number = {2},
pages = {591--616},
title = {{Who Should Be Treated? Empirical Welfare Maximization Methods for Treatment Choice}},
volume = {86},
year = {2018}
}
@unpublished{Yagle,
abstract = {Minimization of the elastic net cost func- tion ||y−Ax||2 2+2$\lambda$||x||1+2µ||x||2 2 arises in reconstruction of sparse signals from noisy observations y of underde- termined linear combinations Ax of x.We reformulate this problem as a non-negative least-squares problem, and solve the latter using Landweber iteration with a non-negativity constraint. In particular, this yields a simple derivation of the thresholded Landweber it- eration for minimization of the LASSO cost function},
author = {Yagle, Andrew E and Statement, A Problem},
keywords = {2-rest,734-763-1503,734-763-9810,aey,edics,edu,eecs,email,fax,phone,sparse reconstruction,umich},
pages = {1--3},
title = {{Elastic Net Minimization as Non-Negative Least Squares using the Landweber Iteration}}
}
@article{Hoderlein2018,
abstract = {This paper proposes a framework to model welfare effects that are associated with a price change in a population of heterogeneous consumers. The framework is similar to that of Hausman and Newey (Econometrica, 1995, 63, 1445–1476), but allows for more general forms of heterogeneity. Individual demands are characterized by a general model that is nonparametric in the regressors, as well as monotonic in unobserved heterogeneity, allowing us to identify the distribution of welfare effects. We first argue why a decision maker should care about this distribution. Then we establish constructive identification, propose a sample counterparts estimator, and analyze its large-sample properties. Finally, we apply all concepts to measuring the heterogeneous effect of a change of gasoline price using US consumer data and find very substantial differences in individual effects across quantiles.},
author = {Hoderlein, Stefan and Vanhems, Anne},
doi = {10.1002/jae.2587},
file = {:home/stefan/Dropbox/Academia/Library/Hoderlein, Vanhems{\_}2018{\_}Estimating the distribution of welfare effects using quantiles.pdf:pdf},
issn = {10991255},
journal = {Journal of Applied Econometrics},
number = {1},
pages = {52--72},
title = {{Estimating the distribution of welfare effects using quantiles}},
volume = {33},
year = {2018}
}
